{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2070dc",
   "metadata": {},
   "source": [
    "# Question 2: Classification [20 Marks]\n",
    "\n",
    "**[NOTE]** For each question, it is possible the your code implementation is correct even if some public test cases seem to fail (due to time-limit, or missing dependencies, or runtime under/overflow). On the hand, even if your code passes all the test cases, you are not guaranteed to be completely correct.\n",
    "\n",
    "**[Background Story, OK to skip]** Intrusion Detection System (IDS) is a common solution in cybersecurity to protect the network from malicious activities. IDS is a solution which is deployed at strategic points in a network which allows it to monitor the incoming and outgoing traffic in search for suspicious/malicious traffic. A type of IDS is anomaly-based IDS, which uses behavior-based detection. By monitoring inbound and outbound network traffic, IDS continuously conducts network traffic analysis and extracts statistical features of network, and alarms if any suspicious activity is detected.\n",
    "\n",
    "**[TASK]** For Question 2, you will use the Intrusion Detection Evaluation Dataset (CICIDS2017) published by University of New Brunswick in co-operation with the Canadian Institute for Cybersecurity to design a simple anomaly-based Intrusion Detection System using machine learning. The task of intrusion detection will be modelled as a **multi-class classification**, which you will train a **Softmax Regressor model** using network features for the classification of different types of network traffic (Safe / Different types of Denial of Service (DoS) attacks). \n",
    "\n",
    "You are given a processed version of the original dataset, which has been cleaned and filtered down to 10 features from the original 78 features. Your exact understanding of each of the features / DoS Attack will not be tested as part of this question, and you are encouraged to focus on the implementation of your ML model and the evaluation of your results using the given metrics.\n",
    "\n",
    "Information on the data features and the target variable is as follows:\n",
    "* Features \n",
    "    * Bwd Packet Length Mean\n",
    "    * Max Packet Length\n",
    "    * Average Packet Size\n",
    "    * Packet Length Mean\n",
    "    * Total Length of Bwd Packets\n",
    "    * Packet Length Std\n",
    "    * Total Fwd Packets\n",
    "    * Bwd Packet Length Std\n",
    "    * Avg Bwd Segment Size\n",
    "    * Packet Length Variance\n",
    "* Target (Classification types of the network)\n",
    "    * Benign (Harmless network traffic)\n",
    "    * DoS GoldenEye\n",
    "    * DoS Hulk\n",
    "    * DoS Slowhttptest\n",
    "    * DoS slowloris\n",
    "        \n",
    "*('Bwd' in this case refers to the Backwork direction of network traffic, and 'Fwd' Forward direction. 'DoS' refers to Denial of Service)*\n",
    "\n",
    "In this question, the dataset has already been split into train and test for you.\n",
    "\n",
    "* Training set data features: X_train\n",
    "* Testing set data features: X_test\n",
    "* Training set target classes: y_train\n",
    "* Testing set target classes: y_test\n",
    "\n",
    "Important point to note is that the target matrix `y` given to you is in the shape of (n, K), where n=number of data points and K=total number of classes. The target has been encoded into a `one-hot encoded` vector, which is a vector representation where all the elements of the vector are 0 expect one, which has 1 as its value corresponding to the index of the class. As an example, given that the set of classes is `(Benign, DoS Golden Eye, Dos Hulk, DoS Slowhttptest, DoS slowloris)`, if `y = Benign`, y can be represented as the following vector `[1,0,0,0,0]`. Similarly, `y = DoS slowloris` is represented as `[0,0,0,0,1]`. The index of the one-hot encoded vector will follow the ordering of this list `[Benign, DoS Golden Eye, Dos Hulk, DoS Slowhttptest, DoS slowloris]`.\n",
    "\n",
    "This understanding will aid your calculations for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4cfc766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08552aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "X_train = np.load('./X_train.npy')\n",
    "y_train = np.load('./y_train.npy')\n",
    "X_test = np.load('./X_test.npy')\n",
    "y_test = np.load('./y_test.npy')\n",
    "\n",
    "# label index of the one hot encoded vector - you may use this as a reference (or as part of your code) when you evaluate \n",
    "# your model using calculated metrics. Otherwise, it is not needed.\n",
    "\n",
    "mapping = {\n",
    "    0:'Benign',\n",
    "    1:'DoS Golden Eye',\n",
    "    2:'DoS Hulk',\n",
    "    3:'DoS Slowhttptest',\n",
    "    4:'DoS slowloris'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4edd5e",
   "metadata": {},
   "source": [
    "*Each sub-question of Question 2 is designed to be graded independently from other parts, hence students are encouraged to attempt as many questions as possible (if not all).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e71a96",
   "metadata": {},
   "source": [
    "### A. Softmax Regression Hypothesis [3 marks]\n",
    "\n",
    "Softmax Regression, more commonly known as multinomial logistic regression, is a generalization of the logistic regression model to multi-class classification settings. By combining your concepts from the logistic regression and the softmax function that you have learnt in CS2109S, you will be able to implement this classfier.\n",
    "\n",
    "In logistic regression, we defined the following hypothesis:\n",
    "\n",
    "$$ h_{w}(x) = \\frac{1}{1+exp(-w^{\\intercal}x)}$$\n",
    "\n",
    "and the corresponding cost function:\n",
    "\n",
    "$$ J(w) = - \\left[ \\sum_{i=1}^{m} y^{(i)}\\log{h_{w}(x^{(i)})} + (1-y^{(i)})\\log{h_{w}(x^{(i)})} \\right] $$\n",
    "\n",
    "We have also learnt about the softmax function, which converts a vector of $K$ real numbers into a probability distribution of $K$ different outcomes, defined as:\n",
    "\n",
    "$$ P(y=j) = \\frac{e^{z_{j}}}{\\sum_{i=0}^{k}e^{z_{i}}}  $$\n",
    "\n",
    "In softmax regression, we will formulate a hypothesis where given a test input $x$, we estimate the probability that $P(y=k|x)$ for each class of $k=1,...,K$. In other words, we want to estimate the probability of the class label taking on each of the $K$ different possible values. This will be done by using the softmax function and our hypothesis will be defined as :\n",
    "\n",
    "$$ \\begin{align}\n",
    "h_{w}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | x; w) \\\\\n",
    "P(y = 2 | x; w) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K | x; w)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{j=1}^{K}{\\exp(w^{(j)\\top} x) }}\n",
    "\\begin{bmatrix}\n",
    "\\exp(w^{(1)\\top} x ) \\\\\n",
    "\\exp(w^{(2)\\top} x ) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp(w^{(K)\\top} x ) \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}  $$\n",
    "\n",
    "To calculate the above hypothesis, we will be using a weight matrix of shape (n,K), where n is the number of data features and K is the total number of classes. Thus, each column $j$ of the weight matrix will be used for the calculation of class $j$, for $j=1,..,K$ different classes.  \n",
    "\n",
    "Lastly, we will ensure that our calculation of the exponential term do not cause numerical overflow during training. \n",
    "\n",
    "Define a function `softmax_regression_hypothesis` which implements this hypothesis function. \n",
    "\n",
    "Hint: Where you need to use the exponential function in numpy, e.g. `np.exp(z)`, you can use `np.exp(z - np.max(z, axis=1, keepdims=True))` to avoid overflow, where z is a numpy matrix. This can be similarly implemented for other packages such as the `math` package at your own discretion. The argument `axis` in `np.max` may also be adjusted depending on your method of calculation. If your implementation of softmax_regression_hypothesis does not take care of numerical overflow correctly, you can still get full credit in this question, but it might affect your training of the Softmax Classifier for the later part of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca6c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression_hypothesis(X, weights):\n",
    "    '''\n",
    "    Computes the hypothesis of the softmax regression model using a given input X, output a vector containing class possibilities \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        - X : matrix containing data features (numpy array of shape (m,n))\n",
    "        - weight : model parameters (numpy array of shape (n,K))\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        - hypothesis : resulting matrix containing probability value of different classes for input x (numpy array of shape (m,K))\n",
    "    '''\n",
    "    \n",
    "    \"\"\"\n",
    "    *Important*\n",
    "    \n",
    "    In your calculation of the exponential term using numpy, please implement np.exp(z - np.max(z, axis, keepdims=True)) in order\n",
    "    to prevent numerical overflow during training\"\n",
    "    \"\"\"\n",
    "    \n",
    "    e_x = np.exp(X@weights - np.max(X@weights, axis=1, keepdims=True) ) # - np.max(X@weights, axis=1, keepdims=True) is added for numerical stability\n",
    "    return (e_x.T / np.sum(e_x,axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca95cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35434369 0.64565631]\n",
      " [0.18242552 0.81757448]]\n",
      "[[0.05957114 0.06583629 0.03613172 0.44017449 0.39828635]\n",
      " [0.09460303 0.10455252 0.07008365 0.38363421 0.34712659]\n",
      " [0.13794432 0.15245205 0.12481718 0.30700072 0.27778574]\n",
      " [0.22059263 0.19960047 0.19960047 0.18060597 0.19960047]\n",
      " [0.19801424 0.17917069 0.16212035 0.21883958 0.24185514]]\n"
     ]
    }
   ],
   "source": [
    "# example execution on sample data\n",
    "\n",
    "test_x_1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "test_weights_1 = np.array([[.1, .2], [.3, .4], [.5, .6]])\n",
    "print(softmax_regression_hypothesis(test_x_1, test_weights_1))\n",
    "# np.array([[0.35434369, 0.64565631],\n",
    "#           [0.18242552, 0.81757448]])\n",
    "\n",
    "test_x_2 = np.array([[6, 5], [4, 3], [2, 1], [0, 1], [1, 2]])\n",
    "test_weights_2 = np.array([[.1, .2, .1, .6, .5], [.3, .2, .2, .1, .2]])\n",
    "print(softmax_regression_hypothesis(test_x_2, test_weights_2))\n",
    "# np.array([[0.05957114, 0.06583629, 0.03613172, 0.44017449, 0.39828635],\n",
    "#           [0.09460303, 0.10455252, 0.07008365, 0.38363421, 0.34712659],\n",
    "#           [0.13794432, 0.15245205, 0.12481718, 0.30700072, 0.27778574],\n",
    "#           [0.22059263, 0.19960047, 0.19960047, 0.18060597, 0.19960047],\n",
    "#           [0.19801424, 0.17917069, 0.16212035, 0.21883958, 0.24185514]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d861c5",
   "metadata": {},
   "source": [
    "### B. Loss function [3 marks]\n",
    "\n",
    "In order to train your softmax regression model, we will implement a generalised form of the cost function which we used to train our logistic regression model. \n",
    "\n",
    "We define our new cost function as follows:\n",
    "\n",
    "$$ J(w) = - \\frac{1}{m}\\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K} \\mathbb{1}_{\\left\\{y^{(i)} = k\\right\\}}  \\log \\frac{\\exp(w^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(w^{(j)\\top} x^{(i)})}\\right] $$\n",
    "\n",
    "where $\\mathbb{1}_{\\left\\{y^{(i)} = k\\right\\}}$ refers to an indicator function that returns a value of 1 if $y^{(i)} = k$ and zero if $y^{(i)} \\neq k$.\n",
    "\n",
    "To allow numerical stability during training, please implement `np.log(z + eps)` in your calculation of the log term using numpy instead of `np.log(z)`.\n",
    "\n",
    "Define a function `loss_fn` which implements this loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf11d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(X, weights, y_true):\n",
    "    '''\n",
    "    Loss function to be used for training of the Softmax Regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        - X : data matrix (numpy array of shape (m,n))\n",
    "        - weights : model parameters (numpy array of shape (n,K))\n",
    "        - y_true : matrix containing one-hot encoded K ground truth targets  (numpy array of shape (m,K))    \n",
    "    Returns\n",
    "    -------\n",
    "        - loss_val : calculated loss value (float) \n",
    "    '''\n",
    "    \n",
    "    \"\"\"\n",
    "    *Important*\n",
    "    \n",
    "    When you implement the log term using numpy, please use np.log(h + eps) instead of just np.log(h) for numerical stability during training, \n",
    "    where eps = np.finfo(np.float64).eps\n",
    "    \"\"\"\n",
    "    eps = np.finfo(np.float64).eps\n",
    "    h = softmax_regression_hypothesis(X, weights)\n",
    "    return -(1/X.shape[0])*np.sum(np.multiply(y_true, np.log(h + eps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1137958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6581"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example execution on sample data\n",
    "\n",
    "test_x = np.array([[6, 5], [4, 3], [2, 1], [0, 1], [1, 2]])\n",
    "test_weights = np.array([[.1, .2, .1, .6, .5], [.3, .2, .2, .1, .2]])\n",
    "test_y = np.array([\n",
    "    [0., 0., 0., 1., 0.], # class 3\n",
    "    [1., 0., 0., 0., 0.], # class 0\n",
    "    [0., 0., 1., 0., 0.], # class 2\n",
    "    [0., 1., 0., 0., 0.], # class 1\n",
    "    [0., 0., 0., 0., 1.], # class 4\n",
    "])\n",
    "round(loss_fn(test_x, test_weights, test_y), 4) # 1.6581"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce88774f",
   "metadata": {},
   "source": [
    "### C. Gradient loss function [2 marks]\n",
    "\n",
    "As we will be training our model using gradient descent, we define the gradient of our cost function as follows:\n",
    "$$ \\nabla_{w^{(k)}} J(w) = -\\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left(  \\mathbb{1}_{\\left\\{y^{(i)} = k\\right\\}}  - P(y^{(i)} = k | x^{(i)}; w) \\right) \\right]  } $$\n",
    "\n",
    "Define a function `grad_loss_fn` which implements this gradient of our loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "417e49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss_fn(X, weights, y):\n",
    "    '''\n",
    "    Gradient of the Cost function to be used for training of the Softmax Regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        - X : data matrix (numpy array of shape (m,n))\n",
    "        - weight : model parameters (numpy array of shape (n,K))\n",
    "        - y : matrix containing one-hot encoded K ground truth targets (numpy array of shape (m,K))\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        - grad : matrix containing calculated partial derivatives of Cost Function J (numpy array of shape (n,K))\n",
    "    '''\n",
    "    \n",
    "    grad_i = -(1/X.shape[0])*np.dot(X.T, y-softmax_regression_hypothesis(X,weights))\n",
    "    return grad_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24306902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55805163  0.25946052 -0.21822407 -0.19831503  0.71513022]\n",
      " [-0.33275396  0.07064658  0.00791358 -0.14458781  0.3987816 ]]\n"
     ]
    }
   ],
   "source": [
    "# example execution on sample data\n",
    "\n",
    "test_x = np.array([[6, 5], [4, 3], [2, 1], [0, 1], [1, 2]])\n",
    "test_weights = np.array([[.1, .2, .1, .6, .5], [.3, .2, .2, .1, .2]])\n",
    "test_y = np.array([\n",
    "    [0., 0., 0., 1., 0.], # class 3\n",
    "    [1., 0., 0., 0., 0.], # class 0\n",
    "    [0., 0., 1., 0., 0.], # class 2\n",
    "    [0., 1., 0., 0., 0.], # class 1\n",
    "    [0., 0., 0., 0., 1.], # class 4\n",
    "])\n",
    "print(grad_loss_fn(test_x, test_weights, test_y))\n",
    "# np.array([[-0.55805163  0.25946052 -0.21822407 -0.19831503  0.71513022]\n",
    "#           [-0.33275396  0.07064658  0.00791358 -0.14458781  0.3987816 ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3658b",
   "metadata": {},
   "source": [
    "### D. Classification [3 marks]\n",
    "\n",
    "Now that you have the hypothesis, loss function and derivative of the loss function, next step will be to train your model.\n",
    "\n",
    "A function that implements the gradient descent algorithm is provided for you. Function ```gradient_descent```, which is provided below, takes in your loss function, derivative of the loss function, and your data to train the classifer. You are to use this generic function for this question, and <b>please do not modify</b> the ```gradient_descent``` function for the purposes of grading.\n",
    "\n",
    "Using your hypothesis, loss and gradient of loss functions derived in Parts A, B and C above, use the following  ```gradient_descent``` function given to train a <b>multi-class classifier</b> using Softmax Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe7eff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function is given to students\n",
    "def gradient_descent(loss_fn, grad_loss_fn, X, y, weights=None):\n",
    "    '''\n",
    "    This function executes training of a linear/logistic regressor model using gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        - loss_fn : function that calculates loss/cost value (function)\n",
    "        - grad_loss_fn : function that calculates the derivative of the loss/cost value(function)\n",
    "        - X : features data matrix for training (numpy array of shape (m,n))\n",
    "        - y : ground truth targets matrix for training (numpy array of shape (m,K)) \n",
    "        - lr : learning rate for updating of weights (float) \n",
    "        - max_iter : maximum number of iteration before termination of gradient descent (int)\n",
    "        - eps : tolerance value for early stopping (float)\n",
    "    Returns\n",
    "    -------\n",
    "        - weights : trained weights (numpy array of shape (n,K))\n",
    "    '''\n",
    "    \n",
    "    # Constant terms. Please do not modify ============\n",
    "    lr=0.01\n",
    "    max_iter=2000\n",
    "    eps=0.0001\n",
    "    # =================================================\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize variables\n",
    "    if weights==None:\n",
    "        # xavier initialization - students need not modify this portion\n",
    "        np.random.seed(2109)\n",
    "        scale = 1/max(1., (2+2)/2.)\n",
    "        limit = math.sqrt(3.0 * scale)\n",
    "        weights = np.random.uniform(-limit, limit, size=(X.shape[1], y.shape[1]))\n",
    "    last_error = np.inf\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        current_error = loss_fn(X,weights,y)\n",
    "        grad = grad_loss_fn(X,weights,y)\n",
    "        weights = weights - lr * grad\n",
    "        if(np.linalg.norm(current_error-last_error)<eps):\n",
    "            break\n",
    "        else:\n",
    "            last_error = current_error\n",
    "\n",
    "    print(f\"Iterations taken: {i}, final training loss: {current_error}, Time taken: {time.time() - start_time}s\")\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864b41d",
   "metadata": {},
   "source": [
    "In particular, define a function ```softmax_regression_classification``` that implements multi-class classification using softmax regression using the weights that you have trained using gradient descent with our implementation of ```gradient_descent``` given above. **Prediction of a class of a single test data value should be chosen via max probability value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb7bb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression_classification(X_train, y_train, X_test, loss_fn, grad_loss_fn):\n",
    "    '''\n",
    "    This function executes training of a softmax regressor model using gradient descent, and conducts \n",
    "    multi-class classification prediction on test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        - X_train : features data matrix for training (numpy array of shape (m,n))\n",
    "        - y_train : one-hot encoded ground truth targets matrix for training (numpy array of shape (m,K))\n",
    "        - X_test : features data matrix for testing (numpy array of shape (z,n), where z refers to the length of test set)\n",
    "        - loss_fn : function that calculates loss/cost value (function)\n",
    "        - grad_loss_fn : function that calculates the derivative of the loss/cost value(function)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        - y_pred : predicted classes by the model using X_test (numpy array of shape (m,))\n",
    "                   Example output: array([0,0,1,2,3,4,2,3,2,...]) where classes 0 to 4 can be mapped back\n",
    "                   to the dictionary `mapping` given at the start of the notebook\n",
    "    '''\n",
    "    trained_weights = gradient_descent(loss_fn, grad_loss_fn, X_train, y_train)\n",
    "    y_pred = softmax_regression_hypothesis(X_test, trained_weights)\n",
    "    return np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14c66ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations taken: 492, final training loss: 1.0286823933260119, Time taken: 3.956742286682129s\n",
      "(4000,)\n",
      "[2 3 2 ... 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Note to students : Run this to train your model and predict on X_test.\n",
    "\n",
    "y_pred = softmax_regression_classification(X_train, y_train, X_test, loss_fn, grad_loss_fn)\n",
    "print(y_pred.shape) # (4000,)\n",
    "print(y_pred) # predicted classes e.g. [2 3 2 ... 4 0 1]\n",
    "\n",
    "# Note: The public test case on Coursemology for this part checks if your `softmax_regression_classification`\n",
    "#       produces the expected predictions using sample `X_train`, `y_train`, `X_test` values, \n",
    "#       and correct implementations of `loss_fn` and `grad_loss_fn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55f9f2",
   "metadata": {},
   "source": [
    "### E. Performance metrics [5 Marks]\n",
    "\n",
    "Now that you have trained our model, it is important that you analyze and understand the performance of your model when tested on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ac116",
   "metadata": {},
   "source": [
    "####  E.1 [2 Marks]\n",
    "\n",
    "Define a function ```confusion_matrix``` that will calculate confusion matrix for $n$ classes. An example of a confusion matrix is as shown belows.\n",
    "```\n",
    "                            Predicted\n",
    "     \n",
    "                      class 1  class 2  class 3  class 4  class 5\n",
    "          class 1   [[   x        x        x        x        x   ]\n",
    " \n",
    "Actual    class 2    [   x        x        x        x        x   ]\n",
    "\n",
    "          class 3    [   x        x        x        x        x   ]\n",
    "\n",
    "          class 4    [   x        x        x        x        x   ]\n",
    "\n",
    "          class 5    [   x        x        x        x        x   ]]\n",
    "\n",
    "```\n",
    "\n",
    "As a reminder, the confusion matrix should have True Positive values at its diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "167d3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_test, y_pred):\n",
    "    '''\n",
    "    This function calculates the confusion matrix for any number of class n\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        - y_test : Testing ground truth targets (numpy array of shape (m,))\n",
    "        - y_pred : Predicted targets by the multiclass classifier (numpy array of shape (m,))\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        - cm : nxn confusion matrix, which n is the number of classes (numpy array)\n",
    "    '''\n",
    "    num_classes = len(np.unique(y_test))\n",
    "    cm = np.zeros((num_classes,num_classes))\n",
    "    for i in range(len(y_test)):\n",
    "        cm[y_test[i]][y_pred[i]] +=1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d50d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 2.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Sample execution\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "sample_y_test = np.array([0, 0, 0, 1, 2, 3, 4])\n",
    "sample_y_pred = np.array([4, 0, 4, 1, 2, 3, 4]) # predicts position 0 and 2 wrongly\n",
    "sample_confusion_matrix = confusion_matrix(sample_y_test, sample_y_pred)\n",
    "print(sample_confusion_matrix)\n",
    "# [[1. 0. 0. 0. 2.]\n",
    "#  [0. 1. 0. 0. 0.]\n",
    "#  [0. 0. 1. 0. 0.]\n",
    "#  [0. 0. 0. 1. 0.]\n",
    "#  [0. 0. 0. 0. 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef5ce8",
   "metadata": {},
   "source": [
    "####  E.2. [3 Marks]\n",
    "\n",
    "We will be using a few key metrics to determine the performance of our model, which are the following:\n",
    "* Precision\n",
    "* Recall\n",
    "* F-beta\n",
    "\n",
    "F-beta score is an extension to the F1 Score, which is a **weighted** harmonic mean of precision and recall - meaning that now you are able to assign weights depending on the importance of precision vs recall. The general formula for non-negative real $\\beta$ is given as follows:\n",
    "$$ F_{\\beta} = \\frac{(1+\\beta^{2}) \\cdot (precision \\cdot recall)}{(\\beta^{2} \\cdot precision) + recall}$$ \n",
    "\n",
    "A smaller beta value, such as 0.5, gives more weight to precision and less to recall, whereas a larger beta value, such as 2.0, gives less weight to precision and more weight to recall in the calculation of the score. F1 score is a case of F-beta where beta equals to 1.\n",
    "\n",
    "Define a function `calculate_metrics` which calculates the values of precision, recall, and F-beta scores for each class by using the y_pred returned from the function that you have derived from part **D**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4718778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(cm, beta=1):\n",
    "    '''\n",
    "    Calculates metrics using the confusion matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        -  cm : nxn confusion matrix, which n is the number of classes (numpy array)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        - precision : Array containing precision scores for class 0, ... , class n (Array)\n",
    "            - Eg. [precision_class1, ... , precision_class5]\n",
    "        - recall : Array containing recall scores for class 0, ... , class n (Array)\n",
    "            - Eg. [recall_class1, ... , recall_class5]\n",
    "        - f-beta : Array containing F-beta scores for class 0, ... , class n (Array)\n",
    "            - Eg. [F_beta_class1, ..., F_beta_class5]\n",
    "    '''\n",
    "    eps = np.finfo(np.float64).eps\n",
    "    precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "    recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "    fbeta = (1+(beta**2))*(np.multiply(precision, recall))/(np.add((beta**2)*precision, recall)+eps)\n",
    "    return precision, recall, fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "106846f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         1.         1.         0.33333333]\n",
      "[0.33333333 1.         1.         1.         1.        ]\n",
      "[0.5 1.  1.  1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "# Sample execution\n",
    "\n",
    "sample_confusion_matrix = np.array([\n",
    "    [1., 0., 0., 0., 2.],\n",
    "    [0., 1., 0., 0., 0.],\n",
    "    [0., 0., 1., 0., 0.],\n",
    "    [0., 0., 0., 1., 0.],\n",
    "    [0., 0., 0., 0., 1.],\n",
    "])\n",
    "sample_precision, sample_recall, sample_fbeta = calculate_metrics(sample_confusion_matrix, beta=1)\n",
    "print(sample_precision)\n",
    "print(sample_recall)\n",
    "print(sample_fbeta)\n",
    "# dummy metrics for test case:\n",
    "# class:      0          1          2          3          4\n",
    "# precision: [1.         1.         1.         1.         0.33333333]\n",
    "# recall:    [0.33333333 1.         1.         1.         1.        ]\n",
    "# fbeta:     [0.5        1.         1.         1.         0.5       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c44f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82063264 0.         0.86704653 0.03184713 0.        ]\n",
      "[0.90023659 0.         0.68646617 0.14705882 0.        ]\n",
      "[0.85859346 0.         0.76626102 0.05235602 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# actual results\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "precision, recall, fbeta = calculate_metrics(cm)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(fbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdad48f",
   "metadata": {},
   "source": [
    "### F. Evaluate and explain [4 marks]\n",
    "\n",
    "Based on the metrics that you have calculated, evaluate your multi-class classifier's performance by answering the following questions in this section.\n",
    "\n",
    "You should provide justifications in relation to the context / data / method wherever necessary, and answer the questions in a **short and concise manner**. \n",
    "\n",
    "We will be awarding marks based on your understanding of metrics, and logical application of those metrics to interpret your results. \n",
    "\n",
    "[4 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7078c4f",
   "metadata": {},
   "source": [
    "#### F.1\n",
    "\n",
    "Is precision *and/or* recall a good measure of performance for an IDS system? \n",
    "Please explain your answer in less than **3 concise sentences**..\n",
    "[2 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44550fbb",
   "metadata": {},
   "source": [
    "#### F.2\n",
    "\n",
    "Explain your model performance scores for the different types of attacks with respect to the precision/recall/F-beta scores which you have calculated. Explain your choice of the beta value as well. Please keep your answer to less than **3 concise sentences**.\n",
    "\n",
    "[2 Marks]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9962d94577a72da81b55e09d9e5284553e9b51ef3328f5dc4eb0655903faae7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
